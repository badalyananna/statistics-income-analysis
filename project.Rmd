---
title: "project"
output: html_document
author: "Anna Badalyan", "Rebecca Di Francesco"
date: "07 06 2022"
output: pdf_document, html_document
---
```{r}
library(ggplot2)
```

Reading the dataset and encoding character variables as factors:

```{r}
curPop <- read.csv("C:\\Users\\Anna\\Documents\\Personal_files\\Uni_DS\\Courses\\Statistical_Learning\\Mod_B\\project\\gender_gap_dataset\\CurrentPopulationSurvey.csv")

dim(curPop)

```
###Cleaning data

The dataset contains 344287 observations and 234 variables.

Let's print the summary of the first 122 variables.
```{r}
summary(curPop[,1:122])

```
We can see that variables with a prefix o_  taken from the original Current Population survey, aren't cleaned, so we will use the version without the prefix. We can also notice that the variables gq, month, popstat, labforce, incbus, incfarm aren't informative as they contain the same value (their min, max and mean are the same), so we can drop them.
Many variables are categorical, but R reads them as numbers. We will need to represent them as factors for further modeling. The list of these variables is the following:
region, statefip, metro, metarea, relate, sex, race, marst, bpl, 
citizen, mbpl, fbpl, nativity, hispan, sch, empstat, classwkr, classwly, occ, ind, occ1990, ind1990, occ1950, ind1950, occly, indly, occ50ly, ind50ly, wkswork2, wkswork1, union, srcearn, ftype

Let's print the summary of the remaining columns:

```{r}
summary(curPop[,123:ncol(curPop)])
```
In the second part of the dataset, most of the variables were generated using the original 60 variables, however, it would be easier to take the occupation and industry variables using the already available dummy variables.

```{r}
curPop$industry[curPop$Agriculture == 1] <- 'Agriculture'
curPop$industry[curPop$miningconstruction == 1] <- 'MiningConstruction'
curPop$industry[curPop$durables == 1] <- 'Durables'
curPop$industry[curPop$nondurables == 1] <- 'Nondurables'
curPop$industry[curPop$Transport == 1] <- 'Transport'
curPop$industry[curPop$Utilities == 1] <- 'Utilities'
curPop$industry[curPop$Communications == 1] <- 'Communications'
curPop$industry[curPop$retailtrade == 1] <- 'RetailTrade'
curPop$industry[curPop$wholesaletrade == 1] <- 'WholesaleTrade'
curPop$industry[curPop$finance == 1] <- 'Finance'
curPop$industry[curPop$SocArtOther == 1] <- 'SocArtOther'
curPop$industry[curPop$hotelsrestaurants == 1] <- 'HotelsRestaurants'
curPop$industry[curPop$Medical == 1] <- 'Medical'
curPop$industry[curPop$Education == 1] <- 'Education'
curPop$industry[curPop$professional == 1] <- 'Professional'
curPop$industry[curPop$publicadmin == 1] <- 'Publicadmin'

curPop$occupation[curPop$manager == 1] <- 'manager'
curPop$occupation[curPop$business == 1] <- 'business'
curPop$occupation[curPop$financialop == 1] <- 'financialop'
curPop$occupation[curPop$computer == 1] <- 'computer'
curPop$occupation[curPop$architect == 1] <- 'architect'
curPop$occupation[curPop$scientist == 1] <- 'scientist'
curPop$occupation[curPop$socialworker == 1] <- 'socialworker'
curPop$occupation[curPop$postseceduc == 1] <- 'postseceduc'
curPop$occupation[curPop$legaleduc == 1] <- 'legaleduc'
curPop$occupation[curPop$artist == 1] <- 'artist'
curPop$occupation[curPop$lawyerphysician == 1] <- 'lawyerphysician'
curPop$occupation[curPop$healthcare == 1] <- 'healthcare'
curPop$occupation[curPop$healthsupport == 1] <- 'healthsupport'
curPop$occupation[curPop$protective == 1] <- 'protective'
curPop$occupation[curPop$foodcare == 1] <- 'foodcare'
curPop$occupation[curPop$building == 1] <- 'building'
curPop$occupation[curPop$sales == 1] <- 'sales'
curPop$occupation[curPop$officeadmin == 1] <- 'officeadmin'
curPop$occupation[curPop$farmer == 1] <- 'farmer'
curPop$occupation[curPop$constructextractinstall == 1] <- 'constructextractinstall'
curPop$occupation[curPop$production == 1] <- 'production'
curPop$occupation[curPop$transport == 1] <- 'transport'

print(unique(curPop$industry))
print(unique(curPop$occupation))

```
We can see, then newly generated values don't contain any null values.

As the year ranges from 1981 to 2013, we will need the inflation variable (*inflate*) and for convenience we will also use *realhrwage* variable.

We can see that we have several variables for income, which seem identical, *incwage*, *niincwage*, *incwageman*.
Let's check if they are identical:
```{r}
sum(data$incwage == data$niincwage) == sum(data$incwage == data$incwageman)
```
We verified, that the variables are identical, so we will only use the *incwage* column.


Another column for wage is *tcincwage*. Let's plot the boxplots to compare it with the *incwage*.
`

```{r}
boxplot(data$incwage, data$tcincwage)
```

We can see that the values are similar for the income below 200 000, however, the *incwage* contains more extreme values. However, as *incwageman* column includes imputed values, we will stick to *incwage*.

Thus, we chose the following columns:
```{r}

data <- curPop[c("year", "serial", "numprec", "wtsupp", "region", "statefip", "metro", "metarea", "county", "pernum", "wtsupp", "relate", "age", "sex", "race", "marst", "bpl", "yrimmig", "citizen", "mbpl", "fbpl", "nativity", "sch","educ99", "schlcoll", "empstat", "occupation", "industry", "classwkr", "occly", "indly", "classwly", "wkswork1", "hrswork", "uhrswork", "union", "incwage", "inclongj", "oincwage", "srcearn","ftype", "quhrswor", "qwkswork", "qinclong", "qincwage", "hrwage", "inflate", "realhrwage")]

```

To have the real income we need to multiply the inflation rate by the income values thus:
```{r}
data$realincwage <- data$incwage*data$inflate
```

Next we represent the necessary columns as factors.

```{r}
col.list <- c("region", "statefip", "metro", "metarea", "county", "relate", "sex", "race", "marst", "bpl", "citizen", "mbpl", "fbpl", "nativity", "sch","educ99", "schlcoll", "empstat", "occupation", "industry", "classwkr", "occly", "indly", "classwly", "union", "srcearn","ftype", "quhrswor", "qwkswork", "qinclong", "qincwage")

for (col in col.list) {
  data[[col]] <- as.factor(data[[col]])
}

summary(data)
```
We can notice now that the source of earnings column (*srcearn*) is not informative as it contains only 47 observations for class 4, while the rest 301861 belong to class 1, so we can omit it.

```{r}
data$srcearn <- NULL
```


Let's print the null values in each column.
```{r}
colSums(is.na(data))
```
We can see that columns *yrimmig*, *citizen* and *schlcoll* contain almost 300000  null values. Column *metarea* is missing almost third of the observations, while it just provides a precise code for a metropolitan area. Column *educ99* contains the same information as *sch*. Thus, thise columns can be removed.

```{r}
data$yrimmig <- NULL
data$citizen <- NULL
data$schlcoll <- NULL
data$metarea <- NULL
data$educ99 <- NULL
data$county <- NULL
data$empstat <- NULL

colSums(is.na(data))
```



Now, we can omit the null values.
```{r}
data <- na.omit(data)
summary(data)


```

###EDA

Let's plot the distribution of the *incwage*.


We can see that the distribution is highly left skewed. Thus, we might need to use the log of income.


```{r}

par(mfrow=c(2,2))
# X-axis grid
x2 <- seq(min(data$realincwage), max(data$realincwage), length = 40)

# Normal curve
fun <- dnorm(x2, mean = mean(data$realincwage), sd = sd(data$realincwage))

# Histogram
hist(data$realincwage, prob = TRUE, col = "yellow",
     ylim = c(0, max(fun)),
     main = "Histogram of income", sub= "Comparison with normal curve")
lines(x2, fun, col = "purple", lwd = 2)

qqnorm(data$realincwage)
qqline(data$realincwage)

data$logrealincwage <-log(data$realincwage)
# X-axis grid
x2 <- seq(min(data$logrealincwage), max(data$logrealincwage), length = 40)

# Normal curve
fun <- dnorm(x2, mean = mean(data$logrealincwage), sd = sd(data$logrealincwage))

# Histogram
hist(data$logrealincwage, prob = TRUE, col = "yellow",
     ylim = c(0, max(fun)),
     main = "Histogram of log-transformed income", sub= "Comparison with normal curve")
lines(x2, fun, col = "purple", lwd = 2)

qqnorm(data$logrealincwage)
qqline(data$logrealincwage)

```

Let's plot the *sch* column for education.

```{r}
barplot(table(data$sch))
```
We can see that there are very few values for people that didn't finish school, so we can group them by elementary, middle and high.

```{r}
levels(data$sch) <- c(levels(data$sch),"nosc", "fsch", "scol", "asoc", "bach", "advd")
data$sch[data$sch == 0] <- 'nosc'
data$sch[data$sch == 1] <- 'nosc'
data$sch[data$sch == 2] <- 'nosc'
data$sch[data$sch == 2.5] <- 'nosc'
data$sch[data$sch == 3] <- 'nosc'
data$sch[data$sch == 4] <- 'nosc'
data$sch[data$sch == 5] <- 'nosc'
data$sch[data$sch == 5.5] <- 'nosc'
data$sch[data$sch == 6] <- 'nosc'
data$sch[data$sch == 7] <- 'nosc'
data$sch[data$sch == 7.5] <- 'nosc'
data$sch[data$sch == 8] <- 'nosc'
data$sch[data$sch == 9] <- 'nosc'
data$sch[data$sch == 10] <- 'nosc'
data$sch[data$sch == 11] <- 'nosc'
data$sch[data$sch == 12] <- 'fsch'
data$sch[data$sch == 13] <- 'scol'
data$sch[data$sch == 14] <- 'asoc'
data$sch[data$sch == 16] <- 'bach'
data$sch[data$sch == 18] <- 'advd'
data$sch <- droplevels(data$sch)
barplot(table(data$sch))
```

Let's have a look at region variable. 
```{r}
barplot(table(data$region))
```
The regions are equally distributed among the given variables.

```{r}
barplot(table(data$statefip))
unique(data$statefip)
```

```{r}
barplot(table(data$metro))
```
```{r}
barplot(table(data$relate))
```
```{r}
hist(data$age)
```
```{r}
barplot(table(data$sex))
```
Both genders are equally represented in the dataset.

```{r}
barplot(table(data$race))
```
```{r}
barplot(table(data$marst))
```
```{r}
barplot(table(data$nativity))
```
```{r}
summary(data$relate)
```
We can see that class 1242 with corresponds to foster children has only 18 observations. In fact, it's impossible to have foster children over 25 y.o. in the household, that's why these values are probably erroneous and we will delete them.

```{r}
data <- data[!(data$relate=="1242"),]
data$relate <- droplevels(data$relate)
summary(data$relate)

```


The class of workers variable is organised into 7 levels:(Self-empl=10, private sector=21, government=24, Federal govt employee=25, State govt employee=27, Local govt employee=28, Unpaid family worker=29). Since self-employed and unpaid family worker are only a small amount of units compared to all the rest we can combine them into a singular class: "Other". 

```{r}
summary(as.factor(data$classwkr))
data <- data[!(data$classwkr=="10" | data$classwkr=="29"),]

data$classwkr <- gsub('24', 'Public sector',data$classwkr)
data$classwkr <- gsub('25', 'Public sector',data$classwkr)
data$classwkr <- gsub('27', 'Public sector', data$classwkr)
data$classwkr <- gsub('28', 'Public sector', data$classwkr)

data$classwkr <- gsub('21', 'Private sector',data$classwkr)

```

```{r}
data$classwkr <- as.factor(data$classwkr)

summary(data$classwkr)

```

```{r}
# calculate the percentages
summary(data$realincwage[data$classwkr=="Private sector"])
```



```{r}
# plotting graph -- why weather data in ggtitle?
ggplot(data, aes(fill = data$sex,
                      y = data$realincwage, x = data$classwkr))+
geom_bar(position = "stack", stat = "identity")+coord_cartesian(ylim = c(5000, 1300999))
ggtitle("Weather Data of 4 Cities !")+
theme(plot.title = element_text(hjust = 0.5))
```

Let's plot the correlation matrix of the categorical variables in our dataset.

```{r}
data.cat <- data[c("region", "statefip", "metro", "relate", "sex", "race", "marst", "nativity", "sch", "occupation", "industry", "classwkr", "union", "ftype")]

chisq.matrix <- function(x) {
  names <- colnames(x);
  ndim <- length(names)
  pvals <- matrix(nrow=ndim, ncol=ndim, dimnames = list(names, names))
  stats <- matrix(nrow=ndim, ncol=ndim, dimnames = list(names, names))
  for (i in 1:ndim) {
    for (j in i:ndim) {
      test <- chisq.test(x[,i],x[,j], simulate.p.value = TRUE)
      pvals[i,j] = test$p.value
      pvals[j,i] = pvals[i,j]
      stats[i,j] = test$statistic
      stats[j,i] = stats[i,j]
    }
  }
  
  return (list("p.values"=pvals, "statistics"=stats))
}

mat <- chisq.matrix(data.cat)
#heatmap(mat$p.values)
heatmap(mat$statistics)
```
We can see theat *region* is correlated with *statefip*, *relate* with *ftype* and *race* with *hispan*. So we will use only 1 of the variables in e ach pair: *region*, *relate* and *race*.



```{r}

data$binaryincome <- as.factor(ifelse(data$realincwage >=60000, 1, 0))
summary(data$binaryincome)

```

### Data Visualization
In the density histogram below we can see the different distributions of income (using *binaryincome*) across different age categories. At the left, we can see that the distribution of high income (over 60k) for different ages is distributed almost like a normal distribution. While the low-middle income distribution has a descending shape. 

```{r}
par(mfrow=c(1,2))

# X-axis grid
x2 <- seq(min(data$age[data$binaryincome==1]), max(data$age[data$binaryincome==1]), length = 40)

# Normal curve
fun <- dnorm(x2, mean = mean(data$age[data$binaryincome==1]), sd = sd(data$age[data$binaryincome==1]))

# Histogram
hist(data$age[data$binaryincome==1], prob = TRUE,
     ylim = c(0, max(fun)),
     main = "Histogram of high income distribution 
     grouped by age", xlab="Age",cex.main=0.9, col = "lightblue")
lines(x2, fun, col = "yellow", lwd = 2)

lines(density(data$age[data$binaryincome==1]), col="red", lwd=2)

hist(data$age[data$binaryincome==0], prob = TRUE, col = "#0000FF",
     ylim = c(0, max(fun)),
     main = "Histogram of low-middle income distribution 
     grouped by age", cex.main=0.9, xlab="Age")

lines(density(data$age[data$binaryincome==0]), col="red", lwd=2)
```

Now, let's check if the age variable grouped by sex is balanced. We want to avoid imbalance because as we noticed above young people tend to have lower income as opposite of older people. Thus if we had more younger males than younger females or viceversa this would bias our analysis. Luckily, it seems that we have a balanced number of males and females for each year of age.

```{r}
# Stacked + percent
ggplot(data, aes(fill=sex, y=age, x=age)) + 
    geom_bar(position="fill", stat="identity")
```


###ANOVA

Let's compare the median income for males and females to see if there is a difference. We want to use the median and not the mean because outlier data can skew the average. As we see, the median income is around 10,000 dollars higher for males. 

```{r}

group_median = aggregate(data$realincwage, list(data$sex), FUN=median) 
colnames(group_median) <- c("Sex", "Median income ($)")
levels(group_median$Sex) <- c("Male","Female")


group_mean = aggregate(data$realincwage, list(data$sex), FUN=mean) 
colnames(group_mean) <- c("Sex", "Average income ($)")
group_mean$Sex <- NULL
cbind(group_median, group_mean)
```

So when considering the mean, the difference in income between the two group is even larger than 10,000 dollars. 
We could perform a one-way ANOVA test to check if there is a statistically significant difference between the mean income for males and females (H1). In this case the continuous income variable *realincwage* is the dependent variable and *sex* is the independent variable. 
The assumption of sample independence can be considered true. It remains to check the normality of residuals and the variance equality assumption.

-Check that variance in the groups is equal

Let's see if variance equality can be perceived visually trough a boxplot. 
The interquartile range for males is larger than the one for females so the variance is slightly higher for the males group. 

```{r}
par(mfrow = c(2,1))
qplot(as.factor(sex), data$realincwage, geom = "boxplot", data = data, na.rm=TRUE, fill=as.factor(sex))+coord_cartesian(ylim = c(10000, 70000))+labs(title = "Mean income by sex", x ="Sex", y="Income level")+ scale_fill_discrete(name = "Sex", labels = c("Males", "Females"))

qplot(as.factor(sex), data$realincwage, geom = "boxplot", data = data, na.rm=TRUE, fill=as.factor(sex))+coord_cartesian(ylim = c(7000, 1000000))+labs(title = "Mean income by sex", x ="Sex", y="Income level")+ scale_fill_discrete(name = "Sex", labels = c("Males", "Females"))
```
We can run a Bartlett’s Test to determine whether or not the income variances between males and females are different. Considering a 0.05 significance level, the p-value of the test is a number way smaller than that so we have evidence to say that the samples do not all have equal variances. 

```{r}
bartlett.test(data$realincwage ~ data$sex)
```
In general, ANOVA’s are considered to be fairly robust against violations of the equal variances assumption as long as each group has the same sample size, which is the case: 

```{r}
summary(data$sex)
```

```{r}
res.aov <- aov(log(data$realincwage) ~ data$sex, data = data)
# Summary of the analysis
summary(res.aov)
```
```{r}
res.aov <- aov(log(data.reg$realincwage) ~ ., data = data.reg)
# Summary of the analysis
summary(res.aov)
```


As the p-value is less than the significance level 0.05, we can conclude that there are significant differences in income between males and females. Still, running an ANOVA test with the assumption of equality of variances that is violated can cause more frequent type I error. Thus let's try with Welch's ANOVA. For normal, different-variance, and balanced data (i.e. same-size samples), Welch’s has the most power and the lowest type I error rate. By looking at the result of this test we can draw the same conclusion as for the ANOVA test. 

```{r}
oneway.test(log(data$realincwage) ~ data$sex, data = data, var.equal = FALSE)
```

- Check that residuals are normally distributed
Clearly from the Q-Q plot below the residuals are not normally distributed however the one-way is considered a robust test against the normality assumption.

```{r}
qqnorm(res.aov$residuals)
qqline(res.aov$residuals)
```



```{r}
group_mean = aggregate(data$realincwage, list(data$sex, data$race), FUN=mean) 
colnames(group_mean) <- c("Sex", "Race", "Average income ($)")
levels(group_mean$Sex) <- c("Male", "Female")
levels(group_mean$Race) <- c("White", "Black", "Hispanic", "Other")

males = group_mean[group_mean$Sex=="Male", ]
males <- males[,2:3]
colnames(males) <- c("Race", "Average male income ($)")
females = group_mean[group_mean$Sex=="Female", ]
females <- females[,2:3]
colnames(females) <- c("Race", "Average female income ($)")

#new_dataf <- data.frame(first_column = group_mean[group_mean$`Average income ($)`&group_mean$Sex=="Male")
total <- merge(males,females,by="Race")

total<- cbind(total, c(total$`Average male income ($)`-total$`Average female income ($)`))
colnames(total)[4] <- "Abs difference in income"
total
```

```{r}
CombinedPlot=ggplot(data, aes(x=as.factor(data$race), y=data$realincwage, fill=as.factor(data$sex)))+  geom_boxplot()+coord_cartesian(ylim = c(9000, 150000))+scale_fill_discrete(name = "Sex", labels = c("Male", "Female"))+ labs(title  = "Boxplot of income grouped by sex and race", x ="Race", y="Income level ($)")+scale_x_discrete(labels=c("1" = "Black", "2" = "Hispanic", "3" = "Other", "4"="White"))

CombinedPlot
```
```{r}
group_mean = aggregate(data$realincwage, list(data$sex, data$sch), FUN=mean) 
colnames(group_mean) <- c("Sex", "Education", "Average income ($)")
levels(group_mean$Sex) <- c("Male", "Female")
levels(group_mean$Education) <- c("No school", "12 years of school ", "Some college", "Associate degree", "Bachelor's degree", "Advanced degree")

males = group_mean[group_mean$Sex=="Male", ]
males <- males[,2:3]
colnames(males) <- c("Education", "Average male income ($)")
females = group_mean[group_mean$Sex=="Female", ]
females <- females[,2:3]
colnames(females) <- c("Education", "Average female income ($)")

#new_dataf <- data.frame(first_column = group_mean[group_mean$`Average income ($)`&group_mean$Sex=="Male")
total <- merge(males,females,by="Education")

total<- cbind(total, c(total$`Average male income ($)`-total$`Average female income ($)`))
colnames(total)[4] <- "Abs difference in income"

total
```

```{r}
CombinedPlot=ggplot(data, aes(x=as.factor(data$sch), y=data$realincwage, fill=as.factor(data$sex)))+  geom_boxplot()+coord_cartesian(ylim = c(9000, 150000))+scale_fill_discrete(name = "Sex", labels = c("Male", "Female"))+ labs(title  = "Boxplot of income grouped by sex and education", x ="Education", y="Income level ($)")+scale_x_discrete(labels=c("nosc" = "No school", "fsch" = "12 years of school", "scol" = "Some college", "asoc"="Associate degree", "bach" = "Bachelor's degree", "advd" = "Advanced degree"))

CombinedPlot
```


###Logistic Regression

```{r}
###LOGISTIC REGRES
data.logistic <- data[, -c(1,4)-6-c(8,10)-c(15,18)-c(23,25)-27-c(30,42)]
size<- round(.8 * dim(data)[1])  # training set size
training_set <- data[1:size,]
testing_set <- data[-(1:size),]

```
summary(training_set)
```{r}
m1 <- glm(training_set$binaryincome ~ training_set$year+training_set$region+training_set$statefip, data = training_set, family = binomial('logit'))
summary(m1)
```

```{r}
sum(is.na(training_set$statefip))
```


## KNN
As the KNN algorithm uses numeric methods, we need to make sure all the variables are numeric.

```{r}
var.knn <- c("year", "numprec", "region", "metro", "relate", "age", "sex", "race", "marst", "nativity", "sch", "occupation", "industry", "classwkr", "union", "wkswork1", "uhrswork")

data.knn <- data[var.knn]

for (var in var.knn) {
  data.knn[[var]] <- as.numeric(data[[var]])
}


set.seed(1)

train <- sample(1:nrow(data.knn), nrow(data.knn)*0.75)
test <- (-train)
y <- data$binaryincome
y.test <- y[test]

```

```{r}
library(class)

knn.pred <- knn(data.knn[train,], data.knn[test,], y[train], k=1)
table(knn.pred,y.test)
```
```{r}
knn.pred <- knn(data.knn[train,], data.knn[test,], y[train], k=3)
table(knn.pred,y.test)
```
```{r}
knn.pred <- knn(data.knn[train,], data.knn[test,], y[train], k=5)
table(knn.pred,y.test)
```


```{r}
knn.pred <- knn(data.knn[train,], data.knn[test,], y[train], k=7)
table(knn.pred,y.test)
```


# Linear regression

Let's define the variables we will use in the regression.

```{r}
data.reg <- data[c("year", "numprec", "region", "metro", "relate", "age", "sex", "race", "marst", "nativity", "sch", "occupation", "industry", "classwkr", "union", "wkswork1", "uhrswork", "realincwage")]

data.reg$year <- scale(data.reg$year)
data.reg$numprec <- scale(data.reg$numprec)
data.reg$age <- scale(data.reg$age)
data.reg$wkswork1 <- scale(data.reg$wkswork1)
data.reg$uhrswork <- scale(data.reg$uhrswork)

set.seed(1)

train <- sample(1:nrow(data.reg), nrow(data.reg)*0.75)
test <- (-train)
y <- log(data.reg$realincwage)
y.test <- y[test]
```


We will build the linear regression to model the *realincwage* parameter.

```{r}
reg.out <- lm(log(realincwage) ~ ., data = data.reg[train,])
summary(reg.out)

```
We can see from the summary, that the full Linear regression model explains 63.32% of variance associated with the response variable. Overall, while some dummy variables are not significant, we can conclude that the model is all predictors are significant as the p-value associated with the F-statistic is highly significant.

The year variable has a positive coefficient, which means that on average, people earn more every year.
The p-value associated with region12 is larger than 0.05, which means that there's no significant difference between the average income in the aforementioned region and the base region.
The negative coefficients for all the significant *relate* levels shows that most people earn less that the head of their household, which was used as a base class.
The coefficient of the sex2 variable shows the same result obtained with anova analysis, there's a significant difference between the earnings of males and females with the later earning less than the former.
Regarding race, white people tend to earn more than others with the Hispanic having lower earnings on average.
Interestingly, married individuals with a present spouse tend to earn more on average than other people.
The positive education coefficients indicate that each subsequent level of education achieved leads to increase in average earnings. We previously fitted the model with different categories for grades finished at school and these didn't have any significant differences with people who didn't attend school.
Regarding occupation, architect was used as a base class and we can see that on average, only managers, healthcare workers, computer workers, lawyers and physician earn more than architects. On average, lawyers and physicians have the highest earnings, while people doing building related jobs have the lowest wage earning.
Regarding industry, the  p-value associated with Hotels and Restaurants, Retain Trade, Social work, arts and other services is larger than 0.05 threshold, which means that there's no significant difference between the mentioned classes and the base class (agriculture).
Public sector workers tend to earn more than private sector workers.


```{r}
par(mfrow=c(2,2))
plot(reg.out)
par(mfrow=c(1,1))
```
The residuals vs fitted values behave well and we don't see any systematic behaviors.
The Q-Q plot shows that the observations don't follow the normal distribution and have fat tails. This could mean that the error terms follow the t-student distribution, however, according tot he CLT, with large number of samples it will behave similarly to the normal distribution.
The scale location plot shows that there is some systematic behavior as the red line goes down a bit in the middle. 


Calculating the MSE on the test set:
```{r}
lm.pred <- predict(reg.out, newdata = data.reg[test, ])
mean((lm.pred - y[test])^2)
```


```{r}
library(car)

vif(reg.out)
```
The variance inflation factor analysis shows that there's not problematic collinearity among the predictors. 

## Forward selection
```{r}
library(leaps)
regfit.fwd <- regsubsets(log(realincwage) ~ . , data=data.reg, method="forward", nvmax=100)
fwd.summary <-summary(regfit.fwd)
plot(regfit.fwd, scale="bic")

```


```{r}
par(mfrow=c(1,3))
plot(fwd.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
n.fwd <- which.min(fwd.summary$bic)
points(n.fwd,fwd.summary$bic[n.fwd],col="red",cex=2,pch=20)

plot(fwd.summary$adjr2,xlab="Number of Variables",ylab="Adj R2",type='l')
n.fwd <- which.max(fwd.summary$adjr2)
points(n.fwd,fwd.summary$adjr2[n.fwd],col="red",cex=2,pch=20)

plot(fwd.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
n.fwd <- which.min(fwd.summary$cp)
points(n.fwd,fwd.summary$cp[n.fwd],col="red",cex=2,pch=20)
par(mfrow=c(1,1))
```
We can see that using BIC, Adj R^2 and AIC as a selection parameter, the subset which includes almost all of the variables is the best. In fact, only some of dummy variables are excluded from the model, which is impossible to implement in practice. Thus, we will stic to the full model.

## Backward selection
```{r}
regfit.bwd <- regsubsets(log(realincwage)~. -numprec, data=data.reg, method="backward", nvmax=100)
bwd.summary <-summary(regfit.bwd)
plot(regfit.fwd, scale="bic")

```
```{r}
par(mfrow=c(1,3))
plot(bwd.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
n.bwd.bic <- which.min(bwd.summary$bic)
points(n.bwd.bic,bwd.summary$bic[n.bwd.bic],col="red",cex=2,pch=20)

plot(bwd.summary$adjr2,xlab="Number of Variables",ylab="Adj R2",type='l')
n.bwd <- which.max(bwd.summary$adjr2)
points(n.bwd,bwd.summary$adjr2[n.bwd],col="red",cex=2,pch=20)

plot(bwd.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
n.bwd <- which.min(bwd.summary$cp)
points(n.bwd,bwd.summary$cp[n.bwd],col="red",cex=2,pch=20)
par(mfrow=c(1,1))
```


```{r}
n.bwd.bic
coef(regfit.bwd,n.bwd.bic)
```
```{r}
summary(regfit.bwd, 83)
```


## Ridge regression
```{r}
library(glmnet)

X <- model.matrix(log(realincwage) ~ . , data = data.reg)
X <- X[,-1]
y.test <- y[test]
grid <- 10^seq(3, -4, length=100)

ridge.mod <- glmnet(X, y, alpha=0, standardize = TRUE)
plot(ridge.mod, label=TRUE)

```
```{r}
cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10, type.measure = "mse")
plot(cv.out)
```

```{r}
bestlam <- cv.out$lambda.min
bestlam

```

```{r}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = X[test, ])
mean((ridge.pred - y.test)^2)
```
```{r}
ridge.pred2 <- predict(ridge.mod, s = 0, newx = X[test, ])
mean((ridge.pred2 - y.test)^2)
```


```{r}
out <- glmnet(X, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)
```
```{r}
coef(out)[,100]
```


## Lasso Regression

```{r}
lasso.mod <- glmnet(X[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod, label=TRUE)
```
```{r}
set.seed(1)
cv.out <- cv.glmnet(X[train,], y[train], alpha=1)
plot(cv.out)
```

```{r}
bestlam <- cv.out$lambda.min
bestlam
lasso.pred <- predict(lasso.mod, s=bestlam, newx=X[test,])
mean((lasso.pred-y.test)^2)
```

```{r}
lasso.pred <- predict(lasso.mod, s=0.01, newx=X[test,])
mean((lasso.pred-y.test)^2)
```
```{r}
i <-50
lasso.mod$lambda[i]
beta.L <- coef(lasso.mod)[,i]
beta.L[beta.L != 0]
```
We can see here that the best lambda 0.00012 doesn't eliminate any variables from that model. This proves our conclusions reached with the forward and backward elimination methods.
By choosing 50th lambda, which is equal to 0.343, we can see that except for the intercept, 2 most important variables are *wkswork1* and *uhrswork*. Thus, we will fit a polynomial regression using these variables. 

## Polynomial regression

```{r}
reg.out <- lm(log(realincwage) ~ . +I(uhrswork^2) + I(wkswork1^2), data = data.reg[train,])
summary(reg.out)
```
We can see that adding 2 square terms increased the R-squared from 0.63 to 0.66. The negative coefficient for *uhrswork^2* and *wkswork1^2* indicates that at some point, there's no additional income caused by working more.
